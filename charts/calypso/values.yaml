# Default values for calypso.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

rook-ceph:
  image:
    prefix: rook
    repository: rook/ceph
    tag: v1.7.2
    pullPolicy: IfNotPresent
  crds:
    enabled: false
  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi
  nodeSelector: {}
  tolerations: []
  unreachableNodeTolerationSeconds: 5
  currentNamespaceOnly: false
  annotations: {}
  ## TRACE, DEBUG, INFO, NOTICE, WARNING, ERROR or CRITICAL
  logLevel: INFO
  rbacEnable: true
  pspEnable: true
  csi:
    enableRbdDriver: true
    enableCephfsDriver: true
    enableGrpcMetrics: false
    #enableCSIHostNetwork: true
    enableCephfsSnapshotter: true
    enableRBDSnapshotter: true
    rbdFSGroupPolicy: "ReadWriteOnceWithFSType"
    cephFSFSGroupPolicy: "None"
    enableOMAPGenerator: false
    # Supported values from 0 to 5. 0 for general useful logs, 5 for trace level verbosity.
    #logLevel: 0
    #rbdPluginUpdateStrategy: OnDelete
    allowUnsupportedVersion: false
    forceCephFSKernelClient: true
    volumeReplication:
      enabled: false
      #image: "quay.io/csiaddons/volumereplication-operator:v0.1.0"
  enableDiscoveryDaemon: true
  cephCommandsTimeoutSeconds: "25"
  allowMultipleFilesystems: true
  ## if true, run rook operator on the host network
  #useOperatorHostNetwork: true
  enableSelinuxRelabeling: true
  hostpathRequiresPrivileged: true
  # Disable automatic orchestration when new devices are discovered.
  disableDeviceHotplug: false
  # Blacklist certain disks according to the regex provided.
  #discoverDaemonUdev: ["nvme0n1"]
  # Whether the OBC provisioner should watch on the operator namespace or not, if not the namespace of the cluster will be used
  enableOBCWatchOperatorNamespace: true

rook-ceph-cluster:
  clusterName: rook-ceph
  operatorNamespace: rook-ceph
  toolbox:
    enabled: true
    image: rook/ceph:v1.7.3
  crds:
    enabled: false
  monitoring:
    enabled: false
  cephClusterSpec:
    cephVersion:
      image: "quay.io/ceph/ceph:v16"
      allowUnsupported: false
    dataDirHostPath: /var/lib/rook
    waitTimeoutForHealthyOSDInMinutes: 10
    mon:
      count: 3
      allowMultiplePerNode: false
    mgr:
      count: 1
    dashboard:
      enabled: false
    crashCollector:
      disable: false
    cleanupPolicy:
      # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
      confirmation: ""
      allowUninstallWithVolumes: true
    removeOSDsIfOutAndSafeToRemove: true
    storage:
      useAllNodes: true
      useAllDevices: true
  
  cephBlockPools:
    - name: ceph-blockpool-hdd
      spec:
        deviceClass: hdd
        failureDomain: osd
        replicated:
          size: 2
        parameters:
          compression_mode: none
      storageClass:
        enabled: true
        name: ceph-block-hdd
        isDefault: true
        allowVolumeExpansion: true
        reclaimPolicy: Delete
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4
    - name: ceph-blockpool-ssd
      spec:
        deviceClass: ssd
        failureDomain: osd
        replicated:
          size: 2
        parameters:
          compression_mode: none
      storageClass:
        enabled: true
        name: ceph-block-ssd
        isDefault: false
        allowVolumeExpansion: true
        reclaimPolicy: Delete
        parameters:
          imageFormat: "2"
          imageFeatures: layering
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4
  
  cephFileSystems:
    - name: ceph-filesystem-hdd
      spec:
        metadataPool:
          deviceClass: hdd
          failureDomain: osd
          replicated:
            size: 2
        dataPools:
          - deviceClass: hdd
            failureDomain: osd
            parameters:
              compression_mode: none
            replicated:
              size: 2
         #- deviceClass: hdd
         #  failureDomain: host
         #  parameters:
         #    compression_mode: none
         #  erasureCoded:
         #    dataChunks: 2
         #    codingChunks: 1
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem-hdd
        reclaimPolicy: Delete
        parameters:
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4
    - name: ceph-filesystem-ssd
      spec:
        metadataPool:
          deviceClass: ssd
          failureDomain: osd
          replicated:
            size: 2
        dataPools:
          - deviceClass: ssd
            failureDomain: osd
            parameters:
              compression_mode: none
            replicated:
              size: 2
         #- deviceClass: ssd
         #  failureDomain: host
         #  parameters:
         #    compression_mode: none
         #  erasureCoded:
         #    dataChunks: 2
         #    codingChunks: 1
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem-ssd
        reclaimPolicy: Delete
        parameters:
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          csi.storage.k8s.io/fstype: ext4

  cephObjectStores: []
